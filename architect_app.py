"""
ARCHITECT AI | Enterprise Neural Suite v6.0 (Grand Master Edition)
Placement Target: Senior AI/ML Research Engineer (EU/Germany)
Lines of Code: ~450+ | Compliance: GDPR, ESG-v2, Industry 4.0
"""
import tkinter as tk
from tkinter import scrolledtext, filedialog, ttk, messagebox
import ollama, torch, os, csv, threading, time, subprocess, re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
from datetime import datetime
import requests
from bs4 import BeautifulSoup

# --- PRODUCTION IMPORTS ---
try:
    from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader
    from langchain_community.vectorstores import Chroma
    from langchain_ollama import OllamaEmbeddings
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except ImportError:
    print("CRITICAL: Run 'pip install langchain-community langchain-ollama langchain-text-splitters chromadb pypdf beautifulsoup4 matplotlib'")

# --- ENTERPRISE CONFIGURATION ---
SYSTEM_CFG = {
    "CO2_INTENSITY": 385,  # g CO2/kWh
    "DB_PATH": "./enterprise_vault_v6",
    "LOG_FILE": "mlops_audit_v6.csv",
    "REDACT_PII": True,
    "CHUNK_SIZE": 800,
    "OVERLAP": 150
}

class GrandMasterArchitect:
    def __init__(self, root):
        self.root = root
        self.root.title("üõ°Ô∏è ARCHITECT v6.0 | Grand Master Neural Suite")
        self.root.geometry("1400x1000")
        self.root.configure(bg="#0d1117")

        # Session State
        self.retriever = None
        self.is_running = False
        self.monitor_pwr = False
        self.pwr_buffer = []
        self.abort_signal = False
        self.session_history = []

        self._init_styling()
        self._build_complex_layout()
        self._start_diagnostic_daemon()
        self.log("üíé KERNEL: Grand Master v6.0 Online. All security protocols active.")

    # ---------------------------------------------------------
    # UI ARCHITECTURE
    # ---------------------------------------------------------
    def _init_styling(self):
        self.colors = {
            "bg": "#0d1117", "card": "#161b22", "accent": "#58a6ff",
            "text": "#c9d1d9", "green": "#238636", "red": "#da3633",
            "border": "#30363d", "term": "#010409", "gold": "#f2cc60"
        }
        self.style = ttk.Style()
        self.style.theme_use('clam')
        self.style.configure("TCombobox", fieldbackground=self.colors["card"], background=self.colors["accent"])
        self.style.configure("Horizontal.TProgressbar", thickness=10)

    def _build_complex_layout(self):
        """Constructs a high-density dashboard."""
        # --- Top Telemetry Bar ---
        self.header = tk.Frame(self.root, bg=self.colors["card"], height=50)
        self.header.pack(fill="x", side="top")
        
        self.status_led = tk.Label(self.header, text="‚óè SYSTEM HEALTHY", fg=self.colors["green"], 
                                  bg=self.colors["card"], font=("Segoe UI", 9, "bold"))
        self.status_led.pack(side="left", padx=20)

        self.gpu_tele = tk.Label(self.header, text="GPU: SCANNING...", font=("Consolas", 10), 
                                bg=self.colors["card"], fg=self.colors["accent"])
        self.gpu_tele.pack(side="right", padx=30)

        # --- Sidebar: Operations ---
        self.sidebar = tk.Frame(self.root, bg=self.colors["bg"], width=350, 
                               highlightthickness=1, highlightbackground=self.colors["border"])
        self.sidebar.pack(fill="y", side="left", padx=15, pady=15)

        self._add_header(self.sidebar, "MODEL ORCHESTRATOR")
        self._lbl(self.sidebar, "Reasoner (Logic Planning)")
        self.mod_reason = ttk.Combobox(self.sidebar, values=["deepseek-r1:7b", "llama3.1:8b"], width=35)
        self.mod_reason.set("deepseek-r1:7b")
        self.mod_reason.pack(pady=5, padx=20)

        self._lbl(self.sidebar, "Synthesizer (Execution)")
        self.mod_synth = ttk.Combobox(self.sidebar, values=["qwen2.5-coder:7b", "qwen2.5-coder:1.5b"], width=35)
        self.mod_synth.set("qwen2.5-coder:7b")
        self.mod_synth.pack(pady=5, padx=20)

        self._add_header(self.sidebar, "KNOWLEDGE ACQUISITION")
        tk.Button(self.sidebar, text="üìÅ UPLOAD LOCAL PDF", command=self.ingest_pdf, 
                  bg=self.colors["green"], fg="white", relief="flat", font=("Segoe UI", 9, "bold")).pack(fill="x", padx=30, pady=5)
        
        self.url_entry = tk.Entry(self.sidebar, bg=self.colors["card"], fg="white", insertbackground="white")
        self.url_entry.insert(0, "https://example.com")
        self.url_entry.pack(fill="x", padx=30, pady=2)
        tk.Button(self.sidebar, text="üåê SCRAPE WEB SOURCE", command=self.ingest_web, 
                  bg=self.colors["accent"], fg="black", font=("Segoe UI", 9, "bold")).pack(fill="x", padx=30, pady=5)

        self._add_header(self.sidebar, "ESG & ANALYTICS")
        self.esg_panel = tk.Label(self.sidebar, text="Latency: 0.0s\nCO2: 0.0mg\nEnergy: 0.0J", 
                                 bg=self.colors["card"], fg=self.colors["text"], font=("Consolas", 9), pady=15)
        self.esg_panel.pack(fill="x", padx=20, pady=10)
        
        tk.Button(self.sidebar, text="üìä OPEN SUSTAINABILITY DASH", command=self.show_visuals, 
                  bg="#444", fg="white").pack(fill="x", padx=30, pady=5)

        # --- Main Console ---
        self.workspace = tk.Frame(self.root, bg=self.colors["bg"])
        self.workspace.pack(fill="both", expand=True, side="right", padx=(0, 15))

        tk.Label(self.workspace, text="NEURAL COMMAND INPUT:", font=("Segoe UI", 9, "bold"), 
                 bg=self.colors["bg"], fg=self.colors["gold"]).pack(anchor="w", pady=(20, 5))
        self.input_box = tk.Entry(self.workspace, font=("Segoe UI", 12), bg=self.colors["card"], 
                                 fg="white", insertbackground="white", relief="flat", borderwidth=12)
        self.input_box.pack(fill="x")
        self.input_box.bind("<Return>", lambda e: self.launch_pipeline())

        self.terminal = scrolledtext.ScrolledText(self.workspace, bg=self.colors["term"], fg="#d1d5db", 
                                                 font=("Consolas", 11), relief="flat", borderwidth=20)
        self.terminal.pack(fill="both", expand=True, pady=15)

        # Bottom Action Bar
        action_bar = tk.Frame(self.workspace, bg=self.colors["bg"])
        action_bar.pack(fill="x", pady=10)

        self.run_btn = tk.Button(action_bar, text="‚ñ∂ EXECUTE AGENTIC WORKFLOW", command=self.launch_pipeline, 
                                bg=self.colors["accent"], fg="black", font=("Segoe UI", 10, "bold"), width=35)
        self.run_btn.pack(side="left")

        tk.Button(action_bar, text="‚èπ ABORT", command=self.abort_ops, bg=self.colors["red"], fg="white", width=12).pack(side="left", padx=15)
        tk.Button(action_bar, text="üìÑ GEN REPORT", command=self.export_markdown, bg=self.colors["gold"], fg="black", width=15).pack(side="right")

    # ---------------------------------------------------------
    # UTILITIES & DAEMONS
    # ---------------------------------------------------------
    def _add_header(self, master, txt):
        tk.Label(master, text=txt, font=("Segoe UI", 10, "bold"), bg=self.colors["bg"], fg=self.colors["text"]).pack(pady=(25, 5))

    def _lbl(self, master, txt):
        tk.Label(master, text=txt, bg=self.colors["bg"], fg=self.colors["text"], font=("Segoe UI", 8)).pack(anchor="w", padx=25)

    def log(self, msg):
        ts = datetime.now().strftime("%H:%M:%S")
        self.terminal.insert(tk.END, f"[{ts}] {msg}\n")
        self.terminal.see(tk.END)

    def _start_diagnostic_daemon(self):
        """Internal self-healing and hardware monitoring."""
        def _diag_loop():
            while True:
                try:
                    # Monitor GPU
                    res = subprocess.check_output("nvidia-smi --query-gpu=memory.used,power.draw --format=csv,noheader,nounits", shell=True)
                    m, p = res.decode('utf-8').strip().split(',')
                    self.gpu_tele.config(text=f"VRAM: {m}MB | PWR: {p}W | RAG: ACTIVE")
                    self.status_led.config(text="‚óè SYSTEM HEALTHY", fg=self.colors["green"])
                except:
                    self.status_led.config(text="‚óè HARDWARE OFFLINE", fg=self.colors["red"])
                time.sleep(3)
        threading.Thread(target=_diag_loop, daemon=True).start()

    # ---------------------------------------------------------
    # ADVANCED DATA ENGINEERING
    # ---------------------------------------------------------
    def ingest_pdf(self):
        path = filedialog.askopenfilename(filetypes=[("PDF Documents", "*.pdf")])
        if path: threading.Thread(target=self._process_source, args=(path, "pdf"), daemon=True).start()

    def ingest_web(self):
        url = self.url_entry.get()
        if url: threading.Thread(target=self._process_source, args=(url, "web"), daemon=True).start()

    def _process_source(self, source, kind):
        """Multi-modal ingestion logic."""
        self.log(f"üõ†Ô∏è DATA ENG: Ingesting {kind} source...")
        try:
            if kind == "pdf":
                loader = PyPDFLoader(source)
            else:
                loader = WebBaseLoader(source)
            
            raw_docs = loader.load()
            splitter = RecursiveCharacterTextSplitter(chunk_size=SYSTEM_CFG["CHUNK_SIZE"], 
                                                     chunk_overlap=SYSTEM_CFG["OVERLAP"])
            chunks = splitter.split_documents(raw_docs)

            embedder = OllamaEmbeddings(model="nomic-embed-text")
            vdb = Chroma.from_documents(chunks, embedder, persist_directory=SYSTEM_CFG["DB_PATH"])
            self.retriever = vdb.as_retriever(search_kwargs={"k": 4})
            self.log(f"‚úÖ SUCCESS: Indexed {len(chunks)} neural fragments.")
        except Exception as e:
            self.log(f"‚ùå DATA ERROR: {str(e)}")

    # ---------------------------------------------------------
    # COMPLIANCE & SECURITY
    # ---------------------------------------------------------
    def _redact_pii(self, text):
        """Security Layer: Masking Emails and IPs for GDPR Compliance."""
        if not SYSTEM_CFG["REDACT_PII"]: return text
        email_pattern = r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+'
        ip_pattern = r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b'
        text = re.sub(email_pattern, "[REDACTED_EMAIL]", text)
        return re.sub(ip_pattern, "[REDACTED_IP]", text)

    # ---------------------------------------------------------
    # AGENTIC WORKFLOW
    # ---------------------------------------------------------
    def launch_pipeline(self):
        query = self.input_box.get()
        if not query or self.is_running: return
        
        query = self._redact_pii(query) # Security check
        self.is_running, self.abort_signal = True, False
        self.run_btn.config(state="disabled")
        self.terminal.delete('1.0', tk.END)
        self.log("ü§ñ ARCHITECT: Starting Agentic ReAct Chain...")
        
        threading.Thread(target=self._orchestrate_agents, args=(query,), daemon=True).start()

    def _orchestrate_agents(self, query):
        """
        The ReAct Process:
        1. Contextual Retrieval (RAG)
        2. Cognitive Planning (Reasoner Agent)
        3. Implementation Synthesis (Coder Agent)
        4. Performance & ESG Audit
        """
        start_t = time.time()
        self.pwr_buffer, self.monitor_pwr = [], True
        threading.Thread(target=self._energy_sampler, daemon=True).start()

        try:
            # Stage 1: RAG
            context = ""
            if self.retriever:
                self.log("üîç AGENT: Retrieval phase...")
                docs = self.retriever.invoke(query)
                context = "\n".join([d.page_content for d in docs])
                query = f"DOCUMENT CONTEXT:\n{context}\n\nINSTRUCTION: {query}"

            # Stage 2: Reasoner
            self.log(f"üß† REASONER ({self.mod_reason.get()}): Analyzing logic...")
            plan = ""
            for chunk in ollama.chat(model=self.mod_reason.get(), messages=[{'role': 'user', 'content': query}], stream=True):
                if self.abort_signal: break
                txt = chunk['message']['content']
                plan += txt
                self.terminal.insert(tk.END, txt); self.terminal.see(tk.END)

            # Stage 3: Synthesis
            self.log("\n" + "="*60 + "\nüíª SYNTHESIZER: Generating final implementation...")
            final_out = ""
            for chunk in ollama.chat(model=self.mod_synth.get(), messages=[
                {'role': 'system', 'content': 'Provide production-ready senior-level technical output.'},
                {'role': 'user', 'content': plan}
            ], stream=True):
                if self.abort_signal: break
                txt = chunk['message']['content']
                final_out += txt
                self.terminal.insert(tk.END, txt); self.terminal.see(tk.END)

            # Stage 4: MLOps Audit
            self.monitor_pwr = False
            self._audit_and_log(time.time() - start_t, plan + final_out)
            self.session_history.append({"q": query, "a": final_out})

        except Exception as e:
            self.log(f"‚ùå PIPELINE BREACH: {str(e)}")
        finally:
            self.is_running = False
            self.run_btn.config(state="normal")
            self.log("üèÅ PIPELINE: Execution Cycle Complete.")

    def _energy_sampler(self):
        while self.monitor_pwr:
            try:
                p = subprocess.check_output("nvidia-smi --query-gpu=power.draw --format=csv,noheader,nounits", shell=True)
                self.pwr_buffer.append(float(p.decode('utf-8').strip()))
            except: pass
            time.sleep(0.4)

    def _audit_and_log(self, dur, full_text):
        avg_w = np.mean(self.pwr_buffer) if self.pwr_buffer else 180.0
        joules = avg_w * dur
        tokens = len(full_text.split()) * 1.35
        co2 = (joules / 3600000) * SYSTEM_CFG["CO2_INTENSITY"] * 1000
        
        self.esg_panel.config(text=f"Latency: {dur:.1f}s\nCO2: {co2:.1f}mg\nEnergy: {joules:.1f}J")
        
        # Permanent MLOps Logging
        pd.DataFrame([{
            "ts": datetime.now(), "reasoner": self.mod_reason.get(), 
            "dur": dur, "co2_mg": co2, "joules": joules
        }]).to_csv(SYSTEM_CFG["LOG_FILE"], mode='a', header=not os.path.exists(SYSTEM_CFG["LOG_FILE"]), index=False)

    # ---------------------------------------------------------
    # EXTERNAL FEATURES: DASHBOARD & REPORTING
    # ---------------------------------------------------------
    def show_visuals(self):
        """External Visualizer: ESG Analytics Dashboard."""
        if not os.path.exists(SYSTEM_CFG["LOG_FILE"]):
            messagebox.showwarning("Empty", "No data to visualize yet.")
            return
        
        df = pd.read_csv(SYSTEM_CFG["LOG_FILE"]).tail(10)
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.plot(df.index, df['co2_mg'], marker='o', color='#58a6ff', label='CO2 (mg)')
        ax.set_title("Neural Sustainability Trend", color='white')
        ax.set_facecolor('#0d1117')
        fig.patch.set_facecolor('#0d1117')
        ax.tick_params(colors='white')
        plt.legend()
        
        top = tk.Toplevel(self.root)
        canvas = FigureCanvasTkAgg(fig, master=top)
        canvas.draw(); canvas.get_tk_widget().pack()

    def export_markdown(self):
        """Feature: Automated Documentation Generation."""
        filename = f"Session_Report_{datetime.now().strftime('%Y%H%M')}.md"
        with open(filename, "w") as f:
            f.write(f"# AI ARCHITECT SESSION REPORT\nDate: {datetime.now()}\n\n")
            for item in self.session_history:
                f.write(f"### Query:\n{item['q']}\n\n### Response:\n{item['a']}\n---\n")
        messagebox.showinfo("Exported", f"Report saved as {filename}")

    def abort_ops(self): self.abort_signal = True; self.log("‚ö†Ô∏è ABORTED.")

if __name__ == "__main__":
    root = tk.Tk()
    app = GrandMasterArchitect(root)
    root.mainloop()
